# Relat√≥rio de Levantamento: Configura√ß√£o de Modelo LLM via Vari√°veis de Ambiente

> "The best way to predict the future is to implement it." - Alan Kay ‚öôÔ∏è

## Resumo Executivo

Este relat√≥rio identifica todas as altera√ß√µes necess√°rias para transformar o modelo LLM hardcoded atual (`qwen2.5-coder:7b`) em um sistema configur√°vel via vari√°veis de ambiente, permitindo flexibilidade na escolha de modelos para diferentes ambientes e casos de uso.

## Situa√ß√£o Atual

### Modelo Hardcoded Identificado

Atualmente o modelo est√° hardcoded em **2 locais principais**:

1. **`internal/ai/ollama.go:171`**
   ```go
   modelName := "qwen2.5-coder:7b"
   ```

2. **`internal/worker/client.go:257`**
   ```go
   "ai_model": "qwen2.5-coder:7b",
   ```

### Arquivos de Teste Afetados

3. **`internal/ai/ollama_test.go:60`**
   ```go
   assert.Equal(t, "qwen2.5-coder:7b", service.modelName)
   ```

## An√°lise de Impacto

### üîç Arquivos que Precisam de Altera√ß√£o

#### 1. **Configura√ß√£o Principal** (`internal/config/config.go`)

**Localiza√ß√£o**: Linhas 115-123 (struct WorkerConfig)

**Altera√ß√µes Necess√°rias**:
- Adicionar campos para configura√ß√£o de modelo LLM na struct `WorkerConfig`
- Implementar carregamento das vari√°veis de ambiente
- Adicionar valores padr√£o apropriados

**Campos a Adicionar**:
```go
type WorkerConfig struct {
    // ... campos existentes ...

    // Configura√ß√£o de Modelo LLM
    DefaultModel         string        // Modelo padr√£o
    FallbackModel        string        // Modelo de fallback
    ModelTimeout         time.Duration // Timeout para opera√ß√µes do modelo
    ModelMaxRetries      int           // M√°ximo de tentativas
    ModelTemperature     float64       // Temperatura do modelo
    ModelMaxTokens       int           // M√°ximo de tokens
}
```

**Implementa√ß√£o de Load()** (Linha ~205):
```go
Worker: WorkerConfig{
    // ... campos existentes ...
    DefaultModel:         getEnv("LLM_DEFAULT_MODEL", "qwen2.5-coder:7b"),
    FallbackModel:        getEnv("LLM_FALLBACK_MODEL", "llama3.2:3b"),
    ModelTimeout:         getDurationEnv("LLM_MODEL_TIMEOUT", 120*time.Second),
    ModelMaxRetries:      getIntEnv("LLM_MODEL_MAX_RETRIES", 3),
    ModelTemperature:     getFloatEnv("LLM_MODEL_TEMPERATURE", 0.7),
    ModelMaxTokens:       getIntEnv("LLM_MODEL_MAX_TOKENS", 2048),
},
```

**Fun√ß√µes Helper Necess√°rias**:
```go
func getDurationEnv(key string, defaultValue time.Duration) time.Duration
func getFloatEnv(key string, defaultValue float64) float64
```

#### 2. **Servi√ßo Ollama** (`internal/ai/ollama.go`)

**Altera√ß√µes Principais**:

**A. Modificar Constructor** (Linha 159):
```go
// Antes
func NewOllamaService(ctx context.Context, baseURL string, logger *logging.Logger) (*OllamaService, error)

// Depois
func NewOllamaService(ctx context.Context, baseURL string, modelName string, logger *logging.Logger) (*OllamaService, error)
```

**B. Remover Hardcode** (Linha 171):
```go
// Remover esta linha
modelName := "qwen2.5-coder:7b"

// Usar o par√¢metro recebido
if modelName == "" {
    return nil, fmt.Errorf("model name cannot be empty")
}
```

**C. Adicionar Valida√ß√£o de Modelo**:
- Implementar fun√ß√£o para validar se o modelo existe no Ollama
- Adicionar fallback autom√°tico se modelo principal falhar

**D. Estrutura Expandida**:
```go
type OllamaService struct {
    logger          *logging.Logger
    baseURL         string
    modelName       string
    fallbackModel   string
    timeout         time.Duration
    maxRetries      int
    temperature     float64
    maxTokens       int
    llm             llms.Model
}
```

#### 3. **Worker Client** (`internal/worker/client.go`)

**Altera√ß√£o Necess√°ria** (Linha 257):
```go
// Antes
"ai_model": "qwen2.5-coder:7b",

// Depois
"ai_model": c.config.Worker.DefaultModel,
```

**Metadata Adicional**:
```go
Metadata: map[string]string{
    "ai_model":        c.config.Worker.DefaultModel,
    "fallback_model":  c.config.Worker.FallbackModel,
    "model_timeout":   c.config.Worker.ModelTimeout.String(),
    "max_tasks":       fmt.Sprintf("%d", c.config.Worker.MaxConcurrentTasks),
    "environment":     c.config.Environment,
},
```

#### 4. **Main Worker** (`cmd/worker/main.go`)

**Altera√ß√£o Necess√°ria** (Linha 51):
```go
// Antes
aiService, err := ai.NewOllamaService(ctx, cfg.Worker.OllamaURL, logger)

// Depois
aiService, err := ai.NewOllamaService(ctx, cfg.Worker.OllamaURL, cfg.Worker.DefaultModel, logger)
```

#### 5. **Arquivos de Teste**

**A. `internal/ai/ollama_test.go`**:
- **Linha 48**: Atualizar chamada `NewOllamaService` para incluir par√¢metro de modelo
- **Linha 60**: Atualizar assertion para usar modelo configur√°vel
- **Linha 152**: Atualizar chamada no teste de integra√ß√£o

**B. Adicionar novos casos de teste**:
- Teste com modelo customizado
- Teste com modelo inv√°lido
- Teste de fallback autom√°tico

#### 6. **Arquivos de Ambiente**

**A. `deployments/environments/.env.example`**:

**Adi√ß√µes Necess√°rias** (ap√≥s linha 42):
```bash
# =============================================================================
# LLM Model Configuration
# =============================================================================
LLM_DEFAULT_MODEL=qwen2.5-coder:7b
LLM_FALLBACK_MODEL=llama3.2:3b
LLM_MODEL_TIMEOUT=120s
LLM_MODEL_MAX_RETRIES=3
LLM_MODEL_TEMPERATURE=0.7
LLM_MODEL_MAX_TOKENS=2048

# Task-specific model configurations (JSON format)
LLM_TASK_TYPE_CONFIGS={"TASK_TYPE_INSIGHT_GENERATION":{"model":"qwen2.5-coder:7b","timeout":"45s"},"TASK_TYPE_WEEKLY_REPORT":{"model":"llama3.2:7b","timeout":"120s"}}

# Model-specific parameters (JSON format)
LLM_MODEL_PARAMS={"qwen2.5-coder:7b":{"temperature":0.7,"max_tokens":2048},"llama3.2:7b":{"temperature":0.6,"max_tokens":4096}}
```

**B. `deployments/environments/production/.env.worker.example`**:

**Substituir Se√ß√£o Ollama** (Linhas 24-29):
```bash
# LLM Model Configuration
LLM_DEFAULT_MODEL=qwen2.5-coder:7b
LLM_FALLBACK_MODEL=llama3.2:3b
LLM_MODEL_TIMEOUT=120s
LLM_MODEL_MAX_RETRIES=3
LLM_MODEL_TEMPERATURE=0.7
LLM_MODEL_MAX_TOKENS=2048

# Ollama Server Configuration
OLLAMA_URL=http://ollama:11434
OLLAMA_TIMEOUT=120s
OLLAMA_MAX_RETRIES=3
```

### üîß Altera√ß√µes Opcionais (Futuras)

#### 7. **Configura√ß√£o Hier√°rquica Avan√ßada**

Para implementa√ß√£o futura conforme TODO Task 0125:

**A. Protocol Buffer** (`proto/worker.proto`):
- Adicionar message `LLMConfig`
- Expandir `RegisterWorkerRequest` com `supported_models`

**B. Banco de Dados**:
- Tabela `user_llm_preferences`
- Configura√ß√µes por tipo de task
- Prefer√™ncias individuais de usu√°rio

**C. API Endpoints**:
- Gerenciamento de prefer√™ncias de modelo
- Listagem de modelos dispon√≠veis
- Configura√ß√£o por task type

## Resumo de Mudan√ßas por Arquivo

| Arquivo | Tipo de Altera√ß√£o | Prioridade | Esfor√ßo |
|---------|-------------------|------------|---------|
| `internal/config/config.go` | Estrutural | Alta | M√©dio |
| `internal/ai/ollama.go` | Funcional | Alta | Alto |
| `internal/worker/client.go` | Simples | Alta | Baixo |
| `cmd/worker/main.go` | Simples | Alta | Baixo |
| `internal/ai/ollama_test.go` | Teste | M√©dia | M√©dio |
| `.env.example` | Configura√ß√£o | Alta | Baixo |
| `.env.worker.example` | Configura√ß√£o | Alta | Baixo |

## Ordem de Implementa√ß√£o Recomendada

### Fase 1: Configura√ß√£o B√°sica
1. **Atualizar `internal/config/config.go`** - Adicionar campos e env loading
2. **Atualizar arquivos `.env`** - Definir vari√°veis de ambiente
3. **Modificar `cmd/worker/main.go`** - Passar par√¢metro de modelo

### Fase 2: Servi√ßo Core
4. **Refatorar `internal/ai/ollama.go`** - Aceitar modelo como par√¢metro
5. **Atualizar `internal/worker/client.go`** - Usar modelo configurado

### Fase 3: Testes e Valida√ß√£o
6. **Corrigir `internal/ai/ollama_test.go`** - Adaptar testes existentes
7. **Adicionar novos testes** - Valida√ß√£o de configura√ß√£o
8. **Teste de integra√ß√£o** - Verificar funcionamento end-to-end

## Valida√ß√£o de Funcionalidades

### Cen√°rios de Teste Necess√°rios

1. **Modelo Padr√£o**: Usar configura√ß√£o default
2. **Modelo Customizado**: Configurar via env var
3. **Modelo Inexistente**: Validar tratamento de erro
4. **Fallback**: Testar recupera√ß√£o autom√°tica
5. **Diferentes Ambientes**: Dev, staging, production

### Verifica√ß√µes de Compatibilidade

- ‚úÖ Retrocompatibilidade com configura√ß√£o atual
- ‚úÖ Graceful degradation se env var n√£o definida
- ‚úÖ Valida√ß√£o de modelo no startup
- ‚úÖ Logs informativos sobre modelo em uso

## Benef√≠cios da Implementa√ß√£o

### Operacionais
- **Flexibilidade de Deploy**: Diferentes modelos por ambiente
- **Testes A/B**: Compara√ß√£o de modelos facilmente
- **Otimiza√ß√£o de Recursos**: Modelos menores para dev/test

### Desenvolvimento
- **Configura√ß√£o Simples**: Via vari√°veis de ambiente
- **Debugging Melhorado**: Logs claros sobre modelo usado
- **Manutenibilidade**: C√≥digo menos acoplado

### Produ√ß√£o
- **Fallback Autom√°tico**: Alta disponibilidade
- **Monitoramento**: M√©tricas por modelo
- **Escalabilidade**: Suporte a m√∫ltiplos workers com modelos diferentes

## Riscos e Mitiga√ß√µes

### Riscos Identificados
1. **Breaking Changes**: Altera√ß√£o na assinatura de fun√ß√µes
2. **Configura√ß√£o Incorreta**: Modelo inexistente
3. **Performance**: Overhead de valida√ß√£o

### Mitiga√ß√µes
1. **Versionamento**: Manter compatibilidade com valores default
2. **Valida√ß√£o**: Startup checks para modelos
3. **Caching**: Valida√ß√£o uma vez por startup

## Conclus√£o

A implementa√ß√£o de configura√ß√£o de modelo via vari√°veis de ambiente √© **altamente vi√°vel** e requer altera√ß√µes em **7 arquivos principais** com esfor√ßo **m√©dio** de desenvolvimento. A mudan√ßa proporcionar√° **flexibilidade significativa** sem comprometer a estabilidade atual do sistema.

**Tempo Estimado**: 2-3 dias de desenvolvimento + 1 dia de testes
**Complexidade**: M√©dia
**Impacto**: Alto benef√≠cio, baixo risco

---

*Relat√≥rio gerado em: 02 de Agosto de 2025*
*Vers√£o do Sistema: feat-session-control branch*
*Status: Pronto para implementa√ß√£o*
